{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e579a817",
   "metadata": {},
   "source": [
    "#  RNN with GloVe Embeddings  \n",
    "In this notebook, we will look at using trained word embeddings for our headlines data in a RNN model. Since good result for word embedding training can be achieved with big generic corpus, so it is usually more efficient to use existing embedding vectors from other resources as long as the semantics will match with your downstream tasks. After this notebook, you will be able to replace the embedding layer with the GloVe embeddings and tune your RNN model for a sementic analysis task.\n",
    "\n",
    "**Outline**\n",
    "- Load and process pretrained GloVe word2vec embeddings\n",
    "- Build a classification RNN using GloVe embeddings\n",
    "- Train and evaluate the RNN for semantic analysis  \n",
    "\n",
    "**Estimated time:** \n",
    " 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc38c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change notebook directory, for Gadi environment only\n",
    "import os\n",
    "working_path = os.path.expandvars(\"/scratch/vp91/$USER/Introduction-to-NLP\")\n",
    "os.chdir(working_path)\n",
    "data_path = '/scratch/vp91/NLP-2024/data/'\n",
    "model_path = '/scratch/vp91/NLP-2024/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4410c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths\n",
    "# working_path = './'\n",
    "# data_path = '../data/'\n",
    "# model_path = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74708104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our tokens back\n",
    "with open(working_path+'tokens.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f38cf",
   "metadata": {},
   "source": [
    "## GloVe Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943b95c2",
   "metadata": {},
   "source": [
    "\n",
    "For this workshop, we use a pretrianed GloVe word vectors that is trained from wikipedia in 2014. GloVe models are trained by using word-word co-occurance statistics from the corpus. We are using the smallest word vectors with 6 billion uncased tokens and about 822MB in size.  \n",
    "\n",
    "Say $x_{ij}$ is the frequency that word $j$ occurs in the context of word $i$, for example, before or after the word $i$ within 10 words. If using $k$ to represent any word, then the frequency for any word to occur in woed $i$ context, we have \n",
    "\n",
    "$$x_{i}=\\sum_{k} x_{ik}$$  \n",
    "\n",
    "and the probability of word $j$ to occur in the context of word $i$ (Co-occurance probability) is \n",
    "$$p_{ij}=P(w_j\\mid w_i)=\\frac{x_{ij}}{x_i}$$\n",
    "\n",
    "Then a function was designed to be equal to $$\\frac{p_{ik}}{p_{jk}}$$ so the process of training is to minimise the loss between the designed the function and the fraction of co-occurrance probability.  \n",
    "\n",
    "[Learn more about GloVe](https://nlp.stanford.edu/projects/glove/)   \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Word vectors is a representation of words co-occurance, so if you want to build a model for ancient literatures, sci-fi novels, hate speeches or other type of text that has very different word-word relationships, you can choose to train your own word vectors by building a word2vec modle in pytorch.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa7f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download GoVe\n",
    "# !curl https://downloads.cs.stanford.edu/nlp/data/wordvecs/glove.6B.zip -o glove.6B.zip\n",
    "# !unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b2e0",
   "metadata": {},
   "source": [
    "Now lets have a look at the word vectors. You can choose different dimensions, 50, 100, 200 or 300 to compare their performance in later tasks. But keep in mind that this will impact the number of dimensions in the neural network as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = pd.read_csv(data_path + 'glove.6B.100d.txt', sep=\" \", quoting=3, header=None, index_col=0)\n",
    "glove.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46a6964",
   "metadata": {},
   "source": [
    "Now we put it in a dictionary so we can look up a word for its vector.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 1.</b> <br>\n",
    "Put glove data in a dictionary, with words being the keys and row vectors being the array values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO\n",
    "glove_embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9280587",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glove_embedding['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f82b6",
   "metadata": {},
   "source": [
    "Now we need to extract the vectors from glove to match with our dataset vocabulary. We achieve this by building a weight matrix with the shape of `len(vocabulary) x word_vector_dimension`. This matrix of weights will then be loaded into our embedding layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038445cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(set([w for t in tokens for w in t]))\n",
    "vocab_size = len(vocab)\n",
    "print('Vocabulary size:{}'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba752f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 2. Complete the function: getGloveWeight( )</b><br>\n",
    "- Input parameter: the vocabulary, glove_embedding dictionary <br><br>\n",
    "- Output parameter: a weights_matrix of shape vocabulary_length x embedding dimmension <br><br>\n",
    "If the word in vocabulary exists in glove_embedding, assign the embedding values to the corresbonding row of weights_matrix; otherwise assign a random normal distribution numpy array.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGloveWeight(vocab, glove_embedding):\n",
    "    \n",
    "    ### TODO 2.1: initialise the empty matrix with the correct shape\n",
    "    weights_matrix =  np.zeros()\n",
    "    \n",
    "    for i, w in enumerate(vocab):\n",
    "        # if word exists in glove embedding, we add the embedding vector to weight matrix\n",
    "        if w in glove_embedding.keys():\n",
    "            \n",
    "            ### TODO 2.2: add the embedding vector to correct place in weight matrix\n",
    "            weights_matrix[i] = \n",
    "        # otherwise we intialise a random variable. \n",
    "        else: \n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(weights_matrix.shape[1], ))\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313edbab",
   "metadata": {},
   "source": [
    "The embedding layer stores the vectors of all vocabulary in a weight matrix, and accepts indexes for words to extract corresponding rows of vetors for each sequence input. The above function created the weight matrix. So we need to create the word_to_idx and idx_to_word dictionaries.  \n",
    "To feed the embeddings to the model later, we also need to make all sequenecs of tokens the same length. For padding shorter sequences, we add the special token `<pad>` to our vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be525329",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_embedding['<pad>'] = np.zeros((1,100))\n",
    "vocab.append('<pad>')\n",
    "\n",
    "idx_to_word = {}\n",
    "for i, w in enumerate(vocab):\n",
    "    idx_to_word[i] = w\n",
    "\n",
    "word_to_idx = {}\n",
    "for i, w in enumerate(vocab):\n",
    "    word_to_idx[w] = i\n",
    "\n",
    "print(word_to_idx['thirtysomething'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5040a3",
   "metadata": {},
   "source": [
    "Now we need to tranform our sequence of tokens to word indexes, also make them all the same length. Here we define sequence length as 15 and pad the short sequences with our special token while truncing the longer sequences.  \n",
    "  \n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 3. Complete the function: formatLength( )</b><br>\n",
    "- Input parameter: the list of list of tokens, expected length <br><br>\n",
    "- Output parameter: the result list of list of tokens that all token lists have the expected length <br><br>\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51e42df",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "def formatLength(l, seq_length):\n",
    "    newl=[]\n",
    "    for i, s in enumerate(l):\n",
    "        \n",
    "        ### TODO 3: padding sequences that have length shorter than seq_length\n",
    "        if len(s)<=seq_length:\n",
    "            new_s =  \n",
    "            \n",
    "        # truncting longer sequences\n",
    "        else:\n",
    "            new_s=s[:seq_length]  \n",
    "        newl.append(new_s)\n",
    "    return newl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f6fabf",
   "metadata": {},
   "source": [
    "Now we transfer our equal-length token lists from words to index using the word_to_index dictionary. Also confirm that all lists have the same length as specified in seq_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4022415a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx_tokens = torch.tensor([[word_to_idx[w] for w in t] for t in formatLength(tokens,seq_length)])\n",
    "[print(len(l)) for l in idx_tokens if len(l)!=seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6ac31",
   "metadata": {},
   "source": [
    "Now let's string above functions together and create the embedding layer with our new weights.\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 4. Complete the function: createEmbeddings( )</b><br>\n",
    "- Input parameter: the list of vocabulary, glove embedding dictionary <br><br>\n",
    "- Output parameter: torch embedding layer with GloVe weights. <br><br>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40bc3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer our tokens to vectors\n",
    "def createEmbeddings(vocab, glove_embedding):\n",
    "    ### TODO 4.1: create weight matrix from GloVe weights by calling getGloveWeight()\n",
    "    weights_matrix = getGloveWeight()\n",
    "    \n",
    "    # load the weight matrix to a nn.Embedding layer, freeze=True means embedding weights will not be trained \n",
    "    ### TODO 4.2: convert weight_matrix to float tensor\n",
    "    emb_weights =  \n",
    "    \n",
    "    ### TODO 4.3: create embedding layer from pretrained weights\n",
    "    emb_layer = \n",
    "    return emb_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0df1b1",
   "metadata": {},
   "source": [
    "**Building blocks:**   \n",
    "- [torch.Tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) \n",
    "- [`torch.nn.Embedding` class method `from_pretrained(embeddings, freeze=True, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False`](https://pytorch.org/docs/stable/_modules/torch/nn/modules/sparse.html#Embedding.from_pretrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a694c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Until now, we introduced static word embeddings, which means the same words will have the same weights in all texts they occur. There is also later developed dynamic word embeddings, which means for each occurence of the same word, the weight vectors depends on the context words around each occurence. You can find out more about <a href=\"https://dl.acm.org/doi/fullHtml/10.1145/3178876.3185999\">dynamic word embeddings</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703ecc08",
   "metadata": {},
   "source": [
    "# Build RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6a2de",
   "metadata": {},
   "source": [
    "## Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4d1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load our cleaned data back\n",
    "with open(working_path + 'data_clean.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc572c30",
   "metadata": {},
   "source": [
    "Since we have transferred our data to vectors, we can call `TensorDataset` to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d26fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.FloatTensor(data.is_sarcastic.to_list())\n",
    "# split train test by 80/20\n",
    "training_data = TensorDataset(idx_tokens[:22895], labels[:22895])\n",
    "test_data = TensorDataset(idx_tokens[22895:], labels[22895:])\n",
    "# DataLoader to iterate and batch data quickly\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c10036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea47a95",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> \n",
    "Play with different batch_size and learning rate later to see the model performance.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4399d6",
   "metadata": {},
   "source": [
    "## Define simpleRNN  \n",
    "Here we use the default embeddings first and Train the RNN. You can switch to the Glove Embeddings later and run below cells again to see the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8183bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, out_size):\n",
    "        super(simpleRNN,self).__init__()        \n",
    "        # TODO use Golve Embeddings, no training on embedding\n",
    "#         self.embeddings  = createEmbeddings(vocab,glove_embedding) \n",
    "\n",
    "        # use original embedding with training\n",
    "        self.embeddings  = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(embedding_dim, \n",
    "                          hidden_size, \n",
    "                          num_layers, \n",
    "                          batch_first = True,\n",
    "                          dropout = 0,\n",
    "                          bidirectional = False\n",
    "                     )\n",
    "        self.out = nn.Linear(hidden_size, out_size)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        batch_size = batch.size()[0]\n",
    "        embed = self.embeddings(batch)\n",
    "        rnn_out, hidden = self.rnn(embed)\n",
    "        result = self.out(rnn_out[:,-1,:])\n",
    "        return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405fab9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059accd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embed_dim = len(glove.columns)\n",
    "hidden_size, num_layers = 50, 1\n",
    "output_size = 1\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = simpleRNN(vocab_size, embed_dim, hidden_size, num_layers,output_size)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75653623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(\"Number of threads: \", torch.get_num_threads())\n",
    "\n",
    "# Training\n",
    "num_epochs, lr = 5, 0.01\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "losses, train_acc= [], []\n",
    "epochs = []\n",
    "\n",
    "model.train()\n",
    "for e in range(num_epochs):\n",
    "    num_correct = 0\n",
    "    for d, l in train_dataloader:\n",
    "        model.zero_grad()\n",
    "        scores = model(d)\n",
    "        pred = torch.round(scores.squeeze())\n",
    "        \n",
    "        correct_tensor = pred.eq(l.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "        loss = loss_function(scores.squeeze(), l)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epochs.append(e)\n",
    "    losses.append(float(loss))\n",
    "    acc = num_correct/len(train_dataloader.dataset)\n",
    "    train_acc.append(acc)\n",
    "    print('Epoch {}, loss: {}, accuracy: {}'.format(e, loss, acc))\n",
    "\n",
    "# plot the results\n",
    "plt.title(\"Training Curve\")\n",
    "plt.plot(losses, label=\"Train\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Training Curve\")\n",
    "plt.plot(epochs, train_acc, label=\"Train\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f1fb34",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "model.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_dataloader:\n",
    "    # calculate loss\n",
    "    scores = model(inputs)\n",
    "    loss = loss_function(scores.squeeze(), labels)    \n",
    "    test_losses.append(loss.item())\n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(scores.squeeze())  # rounds to the nearest integer\n",
    "\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "# avg test loss\n",
    "print(\"Test loss: {}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_dataloader.dataset)\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65489f",
   "metadata": {},
   "source": [
    "> Think:  \n",
    "> What should we do if the model is overfitting?   \n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e65da",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 5. Tuning for better performance</b><br>\n",
    "    1. Change hyperparameters<br>\n",
    "    2. Change batch_size<br>\n",
    "    3. Change optimizor<br>\n",
    "    4. Change nn.RNN parameters\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "**Building blocks:**   \n",
    "- [`torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None)`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)  \n",
    "- [Other optimizors](https://pytorch.org/docs/stable/optim.html#algorithms)  \n",
    "- [nn.RNN parameters](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969bd884",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Challenge:</b> <br>\n",
    "    1. Change num_layers to see the result.<br>\n",
    "    2. Add a dropout layer in the model class to improve the result. <br>\n",
    "    3. Change model to bidirectional, be careful with the shape match! <br>\n",
    "    4. Upgrade the model to LSTM or adding Attention layer if you have extra time.\n",
    "</div>  \n",
    "\n",
    "------------------------------------------  \n",
    "\n",
    "Congratulations! After Notebook 1 and 2, you now can clean text data, make tokens, and load the pre-trained weights to an embedding layer in your RNN. In the [next notebook](3-Q&A_bert.ipynb), we will make a small widget to answer questions from a text corpus using fine-tuned transformers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4f5d7df9eebeb2fce5c7cb4fadb86274017838dfb7de8d5dd5849e5abb02796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
