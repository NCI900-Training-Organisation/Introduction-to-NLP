{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "001d5151",
   "metadata": {},
   "source": [
    "# Topic Modelling with LDA   \n",
    "In this notebook we will use the `gensim` library to perform topic modelling task and find out what are the sarcasm headlines database are about.  We will use the cleaned tokens saved from Notebook 1 as input for our models.  \n",
    " \n",
    "\n",
    "**Outline**  \n",
    "- Create bag of words and tf-idf representations for the documents  \n",
    "- Use LDA model to do topic modelling  \n",
    "- Analyse and visualise topic results  \n",
    "\n",
    "\n",
    "**Estimated time:** \n",
    " 30 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515e66ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change notebook directory, for Gadi environment only\n",
    "### Change notebook directory, for Gadi environment only\n",
    "import os\n",
    "working_path = os.path.expandvars(\"/scratch/vp91/$USER/Introduction-to-NLP/\")\n",
    "os.chdir(working_path)\n",
    "data_path = '/scratch/vp91/NLP-2024/data/'\n",
    "model_path = '/scratch/vp91/NLP-2024/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths\n",
    "# working_path = './'\n",
    "# data_path = '../data/'\n",
    "# model_path = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb2cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.corpora import MmCorpus\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import TfidfModel\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#  Use log to make sure that by the final passes, most of the documents have converged. \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e8608",
   "metadata": {},
   "source": [
    "## Load Tokens from Headlines Dataset \n",
    "Now we still use the smaller data and feed the clean tokens to `Dictionary` class. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Remember, if you want to use your own dataset, you need to do the text preprocess like notebook 1 demonstrates before feeding them to the model.\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9cedc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(data_path + 'Sarcasm_Headlines_Dataset_v2.json',lines=True)\n",
    "data\n",
    "# load our tokens back\n",
    "with open(working_path + 'tokens.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "print(len(tokens),tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b03740f",
   "metadata": {},
   "source": [
    "## Add Bi-grams\n",
    "The result of topic models are weighted tokens for each topic and weighted topic for each document. There is NO topic name from the model, only topic indexes. So it is up to us to understand and explain the topic results.  \n",
    "\n",
    "One way to help us with that is to creat n-grams for our tokens.  \n",
    "\n",
    "N-grams means a phrase made of n words. This will show us the frequently occurred phrases and help us to qualitively understand the topic results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8c56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(tokens, min_count=20)\n",
    "for idx in range(len(tokens)):\n",
    "    for token in bigram[tokens[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            tokens[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913114ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee23ff",
   "metadata": {},
   "source": [
    "## Create Input    \n",
    "Using the `Dictionary` class of the library, we can easily filter the words with extremely low or high frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c16f8a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "texts = tokens\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(texts)\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "# remove gaps in id sequence after words that were removed\n",
    "dictionary.compactify()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd41d0-a540-49d1-9d45-468f497f6e65",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Right click on the left side of the output, select **Enable Scrolling for Outputs**\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6120c46",
   "metadata": {},
   "source": [
    "### Vectorization  \n",
    "Now we vectorize the documents using 2 different frequency counts. Bag-of-words, which is the total frequency count in the corpus, and tf-idf, which is capable of highlighting the uniqueness of a word in relation to the document and the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61971e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf-idf representation of the documents\n",
    "## train the tfidf model\n",
    "tfidf = TfidfModel(corpus) \n",
    "### apply the model to whole corpus\n",
    "corpus_tfidf = tfidf[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac52920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd93e4c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training LDAMulticore  \n",
    "Here we use the `LdaMulticore` class to distribute the training on multiple workers.  \n",
    "\n",
    "`num_topics`  \n",
    "Now we are ready to train the model. But how many topics should we define? It really depends on your data and your interpretation. Let's start trying with 10 topics and see what the results are like.  \n",
    "\n",
    "`chunksize`  \n",
    "It controls how many documents are processed each time for training. For example, it the chunksize is the size of the corpus, then the model will process them in one go. \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Becareful when increasing the chunksize, because it needs to fit in memory. \n",
    "</div>   \n",
    "\n",
    "`passes`   \n",
    "It is equivalent to `epoch` in neural network, specifing how many times we want to go over the corpus.  \n",
    "\n",
    "\n",
    "`iterations`  \n",
    "This controls how many times we want to learn each document. High iterations and passes usually improve the result.  \n",
    "\n",
    "`eval_every`  \n",
    "This will evaluate model log perplexity every n updates. Setting to 1 will slow down the training by 2x. \n",
    "\n",
    "`alpha`  \n",
    "This is A-priori belief on document-topic distibution. Can also be 1D array of length equal to num_topics to denote an asymmetric user defined prior for each topic.  \n",
    "\n",
    "`eta`  \n",
    "This is A-prior belief on topic-word distribution. `auto` learns an asymmetric prior from the corpus.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef3bd05",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 5\n",
    "iterations = 20\n",
    "eval_every = 1  \n",
    "alpha = 'symmetric'\n",
    "eta = 'auto'\n",
    "\n",
    "# Build LDA model\n",
    "lda = LdaMulticore(corpus=corpus, id2word=dictionary,num_topics=num_topics, \n",
    "                   workers = 20,\n",
    "                   chunksize=chunksize, passes=passes, iterations=iterations, \n",
    "                   alpha=alpha, eta = eta,\n",
    "#                    decay=0.9,\n",
    "                   # Topics with a probability lower than this threshold will be filtered out.\n",
    "#                    minimum_probability=0.5,\n",
    "                   # for reproducibility.\n",
    "#                    random_state=100,\n",
    "                   # If True, the model also computes a list of topics, sorted in descending order of most likely topics for each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
    "#                    per_word_topics=True,\n",
    "                   # if per_word_topics is True, this represents a lower bound on the term probabilities.\n",
    "#                    minimum_phi_value = 0.5,\n",
    "                  )\n",
    "\n",
    "ldatopics = [[word for word, prob in topic] for topicid, topic in lda.show_topics(formatted=False)]\n",
    "coh= CoherenceModel(topics=ldatopics, texts=texts, dictionary=dictionary, coherence = 'u_mass')\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
    "print('\\nCoherence Score: ', coh.get_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e854f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 5\n",
    "iterations = 20\n",
    "update_every=1\n",
    "eval_every = 1  \n",
    "alpha = 'symmetric'\n",
    "eta = 'auto'\n",
    "\n",
    "# Build LDA model\n",
    "lda_tfidf = LdaMulticore(corpus=corpus_tfidf, id2word=dictionary,num_topics=num_topics, \n",
    "                   workers = 20,\n",
    "                   chunksize=chunksize, passes=passes, iterations=iterations, \n",
    "                   alpha=alpha, eta = eta,\n",
    "#                    decay=0.9,\n",
    "#                    minimum_probability=0.5,\n",
    "#                    minimum_phi_value = 0.5,\n",
    "#                    random_state=100,\n",
    "#                    per_word_topics=True\n",
    "                  )\n",
    "\n",
    "lda_tfidf_topics = [[word for word, prob in topic] for topicid, topic in lda_tfidf.show_topics(formatted=False)]\n",
    "coh_ifidf = CoherenceModel(topics=ldatopics, texts=texts, dictionary=dictionary, coherence = 'u_mass')\n",
    "print('\\nPerplexity: ', lda.log_perplexity(corpus))\n",
    "print('\\nCoherence Score: ', coh.get_coherence())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cc7807",
   "metadata": {},
   "source": [
    "## Result Analysis  \n",
    "### Topic Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d81053",
   "metadata": {},
   "source": [
    "Now we use the `top_topics()` function to get the coherence score for each topic. This function implements `Umass` measure and lower values means better coherence.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfdee50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_topics = lda.top_topics(corpus, coherence='u_mass')\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b0f40",
   "metadata": {},
   "source": [
    "> **Think**  \n",
    "> Can you interprete each topic based on the top words?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d43abb",
   "metadata": {},
   "source": [
    "### Topic Coherence Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3662df3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# number of top words in each topic to use\n",
    "topw =15\n",
    "topic_coh = lda.top_topics(corpus=corpus, texts=texts, dictionary=dictionary, \n",
    "    window_size=None, coherence='u_mass', topn=topw, processes=- 1)\n",
    "tt=[]\n",
    "c=[]\n",
    "for e in topic_coh:\n",
    "    tt.append(e[0])\n",
    "    c.append(e[1])\n",
    "\n",
    "df_tt = pd.DataFrame(tt)\n",
    "df_coh = pd.DataFrame(c)\n",
    "df_coh.rename(columns={0:'coherence'},inplace=True)\n",
    "# pd.concat([df_tt,df_coh],axis=1)\n",
    "df_coh = df_coh.merge(df_tt,left_index=True,right_index=True)\n",
    "df_coh\n",
    "df_coh.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663051c1",
   "metadata": {},
   "source": [
    "> **Think**  \n",
    "> How is the quality of the topics?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ebc5e8",
   "metadata": {},
   "source": [
    "### Document-Topic Representation\n",
    "Sorted by topic probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a361ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get doc topic info from array\n",
    "dt = lda.get_document_topics(bow = corpus, minimum_probability=0, minimum_phi_value=None, per_word_topics=False)\n",
    "dt = [sorted(e,key = lambda x: x[1],reverse=True) for e in dt ]\n",
    "df_t = pd.DataFrame(dt).rename({0:'Top Topic'},axis=1)\n",
    "df_t3 = df_t.loc[:,:2]\n",
    "# get top topicID from tuple\n",
    "top=[]\n",
    "for doc in dt:\n",
    "    top.append(doc[0][0])\n",
    "df_top = pd.DataFrame(top)\n",
    "\n",
    "df_DT = data\n",
    "df_DT = pd.merge(df_t3,df_DT,how='outer',left_index=True, right_index=True)\n",
    "# # print(list(df_DT.columns.values))\n",
    "# df_DT = df_DT[['Top Topic', 1, 2, 'title', 'clean_abs','_id','DOI','indexed.date-time', 'created.date-time', 'deposited.date-time', 'container-title', 'indexed.month', 'created.month', 'deposited.month', 'Fiscal', 'language']]\n",
    "df_DT = pd.merge(df_top,df_DT,how='outer',left_index=True, right_index=True)\n",
    "df_DT.rename(columns={0:'Top Topic','Top Topic':0},inplace=True)\n",
    "df_DT\n",
    "df_DT.groupby('Top Topic')['headline'].nunique().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3b6fc4",
   "metadata": {},
   "source": [
    "> **Think**  \n",
    "> How are documents distributed?  \n",
    "> If you know the corpus, is the distribution consistant with your understanding?  \n",
    "> Can this help us to change any hyperparameter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae95af9",
   "metadata": {},
   "source": [
    "### Topic-Word Representation\n",
    "Ordered word probabilities in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da33037",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rows -> topics\n",
    "tw = lda.print_topics(num_topics=-1, num_words=15)\n",
    "t = []\n",
    "for i in range(num_topics):\n",
    "    t.append(lda.show_topic(i,topw))\n",
    "pd.DataFrame(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff943e8",
   "metadata": {},
   "source": [
    "> **Think**  \n",
    "> Are there duplicate words with top weight in different topics?  \n",
    "> Is there diversity for words in different topics?  \n",
    "> How should we filter the words when we define the Dictionary to improve the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c14ac",
   "metadata": {},
   "source": [
    "### Visualise Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392f550",
   "metadata": {},
   "source": [
    "> **Think**  \n",
    "> Are topics separeted or overlapped?\n",
    "> Which topics might be merged based on their location and key words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f8382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCoA scaling, not good at dissimmilarity\n",
    "pyLDAvis.enable_notebook()\n",
    "# mmds scaling\n",
    "gensimvis.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36aa5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4f5d7df9eebeb2fce5c7cb4fadb86274017838dfb7de8d5dd5849e5abb02796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
