{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question Answering And Semantic Search   \n",
    "The fine-tuning part of this notebook is adapted from [HuggingFace example](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb).    \n",
    "One of the go-to libraries for transformer models is [Hugging Face](https://huggingface.co/docs). \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this workshop, we have downloaded the models to use. If the model loading cells are ran locally it will download the dataset and models through the internet if there is no cache found in the cache_dir. \n",
    "</div>  \n",
    "\n",
    "As we introduced in the lecture, the pre-trained models are pushing NLP field to a new time. In this notebook we will show you how to make a simple widget to get answers from the corpus by semantic search and fine-tuned models. If you can’t find a model for your use-case, you’ll need to finetune a pretrained model on your data. The **Challenge** section demonstrates the steps to fine-tune a model for Q&A task.\n",
    "\n",
    "**Outline**  \n",
    "\n",
    "- Use the Bert model to get answer\n",
    "- Semantic search for articles that are relevant to the question in corpus\n",
    "- The Q&A widget  \n",
    "- Challenge: Fine tune Bert\n",
    "    - Load pre-trained models from Hugging Face library\n",
    "    - Fine tune the bert model for Q&A task using squad data\n",
    "\n",
    "**Estimated time:** \n",
    " 45 mins (excluding challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change notebook directory, for Gadi environment only\n",
    "import os\n",
    "working_path = os.path.expandvars(\"/scratch/vp91/$USER/Introduction-to-NLP/\")\n",
    "os.chdir(working_path)\n",
    "data_path = '/scratch/vp91/NLP-2024/data/'\n",
    "model_path = '/scratch/vp91/NLP-2024/model/'\n",
    "\n",
    "# without setting to false, huggingface will throw a warning incase deadlock occurs. We use small dataset here so we do this here to hide the warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths\n",
    "# working_path = './'\n",
    "# data_path = '../data/'\n",
    "# model_path = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "import sentence_transformers\n",
    "import IPython\n",
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Q&A Model \n",
    "HuggingFace provides a pipeline for easy interence using the models. Here is an example for using the pipeline with our specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we directly use fine-tuned tokenizer and model. Below cell download and save the models. Here we can load from the download directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # download fine-tuned model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# # save to directory\n",
    "# tokenizer.save_pretrained(\"../model/fine-tuned/tokenizer/\")\n",
    "# model.save_pretrained(\"../model/fine-tuned/bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"fine-tuned/tokenizer/\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_path + \"fine-tuned/bert/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell demonstrates how to use the model and tokenizer for Q&A task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswer(contexts, questions, tokenizer, model):\n",
    "    print('>>>> Looking for answers in {} documents...'.format(len(contexts)))\n",
    "    t=time.time()\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "         for context in contexts:\n",
    "            inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "            # word to id representation\n",
    "            input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "            #This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "            outputs = qa_model(**inputs)\n",
    "            answer_start_scores = outputs.start_logits\n",
    "            answer_end_scores = outputs.end_logits\n",
    "\n",
    "            # Get the most likely beginning of answer with the argmax of the score\n",
    "            answer_start = torch.argmax(answer_start_scores)\n",
    "            # Get the most likely end of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) +1\n",
    "            # Get the answer string based on start and end token id\n",
    "            answer = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "            )\n",
    "            answers.append(answer)\n",
    "    print('>>>> Answers extracted in : {}s'.format(time.time()-t))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try with our simple example\n",
    "questions = ['What is extractive question answering?']\n",
    "context = [r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"]\n",
    "getAnswer(context, questions, tokenizer, qa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search in Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model can find the answer sentences in a context. But how can we query the entire corpus? We need to perform semantic search to find the documents that is most relevant to our question/query. To do this, we need to produce embeddings for our corpus as well as queries when we pass them in. Then we can calculate the similarity/distance between question and each document in our corpus to find the relevant ones.  \n",
    "The `sentence_transformer` package provides the models we are using today, trained on 215M question-answer pairs and perform well across search tasks and domains.  \n",
    "![semanticsearch](../img/semanticsearch.png)  \n",
    "image from: https://www.sbert.net/examples/applications/semantic-search/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "bi_encoder = SentenceTransformer(model_name, cache_folder= model_path + 'bi_encoder')\n",
    "\n",
    "query_embedding = bi_encoder.encode('How big is London')\n",
    "passage_embedding = bi_encoder.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Q&A Widget for Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "Now we load the downloaded simple wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "wikipedia_filepath = data_path + 'simplewiki-2020-11-01.jsonl.gz'\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append([data['title'], paragraph])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(passages), passages[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the dataset like we saw in the example above. For time sake we load the existing embedding from data folder. For this model and wiki dataset it took 2 hours to do embedding with 20 workers. This model is fine-tuned from a variation of Microsoft MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model and embedding path is defined, load existing embedding and texts\n",
    "if model_name == 'multi-qa-MiniLM-L6-cos-v1':\n",
    "    embeddings_filepath = data_path + 'corpus_emb_MiniLM-L6.pkl'\n",
    "    if os.path.isfile(embeddings_filepath):\n",
    "        \n",
    "        with open(embeddings_filepath, \"rb\") as fIn:\n",
    "            cache_data = pickle.load(fIn)\n",
    "            passages = cache_data['sentences']\n",
    "            corpus_embeddings = cache_data['embeddings']\n",
    "            corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
    "# otherwise generate new embedding and store it to pickle        \n",
    "else: \n",
    "    corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "    with open(working_path + 'corpus_emb_MiniLM-L6.pkl', \"wb\") as f:\n",
    "        pickle.dump({'sentences': passages, 'embeddings': corpus_embeddings}, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small corpus (up to 1 million documents), we can compute the cosine-similarity between query and documents in corpus by ` util.cos_sim() ` and retrieve top k documents by `torch.topk `. Fortunately this is done for us in `sentence_transformers.util.semantic_search(query_embeddings: torch.Tensor, corpus_embeddings: torch.Tensor, query_chunk_size: int = 100, corpus_chunk_size: int = 500000, top_k: int = 10, score_function: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = <function cos_sim>)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 1. Try it out</b> <br>\n",
    "Embed the quesiton and use the function util.semantic_search( ) and get top_k relevant documents. <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top_k = 5\n",
    "\n",
    "query = 'How many people Aileen Carol killed?'\n",
    "### TODO \n",
    "#  creat embedding for this query using bi_encoder\n",
    "query_embedding = \n",
    "# call semantic search with the top_k variable \n",
    "hits = \n",
    "\n",
    "hits = hits[0]\n",
    "print(\"Input question:\", query)\n",
    "for hit in hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']]))\n",
    "print(\"\\n\\n========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pack above code into a function to use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 2.</b> <br>\n",
    "Write the function searchContext( ): <br>\n",
    "1. Encode the query<br>\n",
    "2. Perform semantic search between question embedding and corpus embedding<br>\n",
    "3. Return top_k hit <b>passages indexes</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchContext(top_k, bi_encoder, query, corpus_embedding):\n",
    "    indexes = []\n",
    "    t = time.time()\n",
    "    ### TODO\n",
    "    #  creat embedding for input query using bi_encoder\n",
    "    question_embedding = \n",
    "    # return a list of indexes for the list of hits extracted\n",
    "    hits = \n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function to get the context from top hit ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the top ranked passages to feed BERT\n",
    "def getContext(indexes, passages):\n",
    "    contexts = []\n",
    "    for k in indexes:\n",
    "        contexts.append(passages[k][1])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Widget 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 3.</b> <br>\n",
    "Complete the function QandA( ): <br>\n",
    "1. Extract top matching document ids, get the contexts and answers<br>\n",
    "2. display original documents that are matched<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QandA(corpus_embeddings, passages):\n",
    "    # Ask for question input\n",
    "    promptQ = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:')\n",
    "    display(promptQ)\n",
    "    question = input()\n",
    "    questions = [question]\n",
    "\n",
    "    # ask for top k value\n",
    "    promptK = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>')\n",
    "    display(promptK)\n",
    "    top_k = int(input())\n",
    "\n",
    "    # display question\n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: '+question+'</div>'\n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    ### TODO 3.1 search the corpus for relevent passages and answers, using searchContext(),getContext() and getAnswer() functions\n",
    "    top_k_ids = \n",
    "    contexts = \n",
    "    answers = \n",
    "    \n",
    "    for a in answers:\n",
    "        answers_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: '+a+'</div>'\n",
    "        display(HTML(answers_HTML))\n",
    "    # warning text\n",
    "    warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>'\n",
    "    display(HTML(warning_HTML))\n",
    "    \n",
    "    ### TODO 3.2 get the list of original documents from search results\n",
    "    doc = \n",
    "    df_hits = pd.DataFrame(doc, columns=['title','text'])\n",
    "    df_hits.text.str.wrap(100)\n",
    "    display(HTML(df_hits.to_html(render_links=True, escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QandA(corpus_embeddings, passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Improve The Widget  \n",
    "### Retrieve & Re-rank with Cross-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are some different ways to improve this workflow. Here we introduce the **Retrieve & Re-rank Pipeline**, which provides better performance for long docment and complex searches. It retrieves the relevant passages first using bi-encoder (what we did above), then re-rank them by the classification score between each pair of question and passage.  \n",
    "![bi-cross encode](../img/Bi_vs_Cross-Encoder.png )   \n",
    "image from: https://www.sbert.net/examples/applications/cross-encoder/README.html  \n",
    "Cross-encoders do not produce embeddings, so it is less efficient for comparision with millions of pairs data. However, in this pipeline, we limit our scope using bi-encoder first and then use cross-encoder to improve the accuracy of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "# downloading from library\n",
    "# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "# cross_encoder.save('../model/cross_encoder')\n",
    "cross_encoder = CrossEncoder(model_path + 'cross_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "query = 'When is Aileen Wuornos born'\n",
    "\n",
    "# The cross-encoder takes 2 inputs and perform classification tasks\n",
    "cross_input = [[query, 'Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".'],\n",
    "                [query, 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.'],\n",
    "                [query, 'Wuornos was diagnosed with antisocial personality disorder and borderline personality disorder.'],\n",
    "                [query, 'The movie, \"Monster\" is about her life. Two documentaries were made about her.'], \n",
    "                [query, 'Wuornos was born Aileen Carol Pittman in Rochester, Michigan. She never met her father. Wuornos was adopted by her grandparents. When she was 13 she became pregnant. She started working as a prostitute when she was 14.']]\n",
    "\n",
    "cross_scores = cross_encoder.predict(cross_input)\n",
    "cross_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 4: Complete function rerank( ) and QandArerank( )</b> <br>\n",
    "    1.  The function takes one question and compares with a list of contexts. We use the list of indexs and passages data to get the context. The list of indexes will be passed by bi-encoder search. The return will be indexes list sorted by cross-encoder score.<br>\n",
    "    2. use bi-encoder to search for top 20 relevant passages and use rerank( ) to get the top_k passage indexes. top_k is the user input number.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, indexs, passages):\n",
    "    print('>>>> Re-ranking results...')\n",
    "    ### TODO 4.1\n",
    "    cross_in = \n",
    "    cross_result = \n",
    "    rank_top = []\n",
    "    for i, v in enumerate(indexs):\n",
    "        rank_top.append([v, cross_result[i]])\n",
    "    rank_top = sorted(rank_top, key=lambda x: x[1], reverse=True)\n",
    "    rank_top = [e[0] for e in rank_top]\n",
    "    return rank_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QandArerank(corpus_embeddings, passages):\n",
    "    # Ask for question input\n",
    "    promptQ = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:')\n",
    "    display(promptQ)\n",
    "    question = input()\n",
    "    questions = [question]\n",
    "\n",
    "    # ask for top k value after cross-encoder\n",
    "    promptK = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>')\n",
    "    display(promptK)\n",
    "    top_k = int(input())\n",
    "\n",
    "    # display question\n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: '+question+'</div>'\n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    ### TODO 4.2 \n",
    "    bi_top_k = \n",
    "    cross_top_k = \n",
    "    contexts = getContext(cross_top_k, passages)\n",
    "    answers = getAnswer(contexts, questions, tokenizer, qa_model)\n",
    "    \n",
    "    for a in answers:\n",
    "        answers_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: '+a+'</div>'\n",
    "        display(HTML(answers_HTML))\n",
    "    # warning text\n",
    "    warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>'\n",
    "    display(HTML(warning_HTML))\n",
    "    \n",
    "    doc = [passages[k] for k in cross_top_k]\n",
    "    \n",
    "    df_hits = pd.DataFrame(doc, columns=['title','text'])\n",
    "    df_hits.text.str.wrap(100)\n",
    "    display(HTML(df_hits.to_html(render_links=True, escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QandArerank(corpus_embeddings, passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the dataset and ask different questions to see the performance  \n",
    "\n",
    ">Pay attention to the **first** answer in both of your widgets, can you see the improvement?  \n",
    "\n",
    ">Change to bigger dataset for longer documents, and try these widgets at home!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Apart from the retrieve-re-rank pipeline, there is also other methods like using Elasticsearch, FAISS indexing or nearest neightbours to improve the workflow. It depends on the size of dataset and the task you wish to perform.  \n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------  \n",
    "By the end of this notebook, you have created and improved a small widget to find answers for you in the articles, i.e information retrieval. In the [next notebook](4-Topic_Modelling.ipynb), we will have a look at topic modelling, another useful application to explore a large amount of text data.    \n",
    "\n",
    "--------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4f5d7df9eebeb2fce5c7cb4fadb86274017838dfb7de8d5dd5849e5abb02796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
