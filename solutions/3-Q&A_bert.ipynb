{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Question Answering And Semantic Search   \n",
    "The fine-tuning part of this notebook is adapted from [HuggingFace example](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb).    \n",
    "One of the go-to libraries for transformer models is [Hugging Face](https://huggingface.co/docs). \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "For this workshop, we have downloaded the models to use. If the model loading cells are ran locally it will download the dataset and models through the internet if there is no cache found in the cache_dir. \n",
    "</div>  \n",
    "\n",
    "As we introduced in the lecture, the pre-trained models are pushing NLP field to a new time. In this notebook we will show you how to make a simple widget to get answers from the corpus by semantic search and fine-tuned models. If you can’t find a model for your use-case, you’ll need to finetune a pretrained model on your data. The **Challenge** section demonstrates the steps to fine-tune a model for Q&A task.\n",
    "\n",
    "**Outline**  \n",
    "\n",
    "- Use the Bert model to get answer\n",
    "- Semantic search for articles that are relevant to the question in corpus\n",
    "- The Q&A widget  \n",
    "- Challenge: Fine tune Bert\n",
    "    - Load pre-trained models from Hugging Face library\n",
    "    - Fine tune the bert model for Q&A task using squad data\n",
    "\n",
    "**Estimated time:** \n",
    " 45 mins (excluding challenge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change notebook directory, for Gadi environment only\n",
    "import os\n",
    "working_path = os.path.expandvars(\"/scratch/vp91/$USER/AI-ML-Applications-Natural-Language-Processing/\")\n",
    "os.chdir(working_path)\n",
    "data_path = '/scratch/vp91/NLP-2023/data/'\n",
    "model_path = '/scratch/vp91/NLP-2023/model/'\n",
    "# without setting to false, huggingface will throw a warning incase deadlock occurs. We use small dataset here so we do this here to hide the warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local paths\n",
    "# working_path = './'\n",
    "# data_path = '../data/'\n",
    "# model_path = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering\n",
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "import sentence_transformers\n",
    "import IPython\n",
    "from IPython.core.display import display, HTML\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Q&A Model \n",
    "HuggingFace provides a pipeline for easy interence using the models. Here is an example for using the pipeline with our specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we directly use fine-tuned tokenizer and model. Below cell download and save the models. Here we can load from the download directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # download fine-tuned model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "# # save to directory\n",
    "# tokenizer.save_pretrained(\"../model/fine-tuned/tokenizer/\")\n",
    "# model.save_pretrained(\"../model/fine-tuned/bert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"fine-tuned/tokenizer/\")\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_path + \"fine-tuned/bert/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell demonstrates how to use the model and tokenizer for Q&A task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswer(contexts, questions, tokenizer, model):\n",
    "    print('>>>> Looking for answers in {} documents...'.format(len(contexts)))\n",
    "    t=time.time()\n",
    "    answers = []\n",
    "    for question in questions:\n",
    "         for context in contexts:\n",
    "            inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "            # word to id representation\n",
    "            input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "            #This outputs a range of scores across the entire sequence tokens (question and text), for both the start and end positions.\n",
    "            outputs = qa_model(**inputs)\n",
    "            answer_start_scores = outputs.start_logits\n",
    "            answer_end_scores = outputs.end_logits\n",
    "\n",
    "            # Get the most likely beginning of answer with the argmax of the score\n",
    "            answer_start = torch.argmax(answer_start_scores)\n",
    "            # Get the most likely end of answer with the argmax of the score\n",
    "            answer_end = torch.argmax(answer_end_scores) +1\n",
    "            # Get the answer string based on start and end token id\n",
    "            answer = tokenizer.convert_tokens_to_string(\n",
    "                tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    "            )\n",
    "            answers.append(answer)\n",
    "    print('>>>> Answers extracted in : {}s'.format(time.time()-t))\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Looking for answers in 1 documents...\n",
      ">>>> Answers extracted in : 0.9121119976043701s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the task of extracting an answer from a text given a question']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try with our simple example\n",
    "questions = ['What is extractive question answering?']\n",
    "context = [r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
    "\"\"\"]\n",
    "getAnswer(context, questions, tokenizer, qa_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Search in Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model can find the answer sentences in a context. But how can we query the entire corpus? We need to perform semantic search to find the documents that is most relevant to our question/query. To do this, we need to produce embeddings for our corpus as well as queries when we pass them in. Then we can calculate the similarity/distance between question and each document in our corpus to find the relevant ones.  \n",
    "The `sentence_transformer` package provides the models we are using today, trained on 215M question-answer pairs and perform well across search tasks and domains.  \n",
    "![semanticsearch](../img/semanticsearch.png)  \n",
    "image from: https://www.sbert.net/examples/applications/semantic-search/README.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: tensor([[0.5472, 0.6330]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "bi_encoder = SentenceTransformer(model_name, cache_folder= model_path + 'bi_encoder')\n",
    "\n",
    "query_embedding = bi_encoder.encode('How big is London')\n",
    "passage_embedding = bi_encoder.encode(['London has 9,787,426 inhabitants at the 2011 census',\n",
    "                                  'London is known for its finacial district'])\n",
    "\n",
    "print(\"Similarity:\", util.cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Q&A Widget for Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset \n",
    "Now we load the downloaded simple wiki dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "wikipedia_filepath = data_path + 'simplewiki-2020-11-01.jsonl.gz'\n",
    "passages = []\n",
    "with gzip.open(wikipedia_filepath, 'rt', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        data = json.loads(line.strip())\n",
    "        for paragraph in data['paragraphs']:\n",
    "            # We encode the passages as [title, text]\n",
    "            passages.append([data['title'], paragraph])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509663 [['Ted Cassidy', 'Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".'], ['Aileen Wuornos', 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.'], ['Aileen Wuornos', 'Wuornos was diagnosed with antisocial personality disorder and borderline personality disorder.'], ['Aileen Wuornos', 'The movie, \"Monster\" is about her life. Two documentaries were made about her.'], ['Aileen Wuornos', 'Wuornos was born Aileen Carol Pittman in Rochester, Michigan. She never met her father. Wuornos was adopted by her grandparents. When she was 13 she became pregnant. She started working as a prostitute when she was 14.']]\n"
     ]
    }
   ],
   "source": [
    "print(len(passages), passages[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we encode the dataset like we saw in the example above. For time sake we load the existing embedding from data folder. For this model and wiki dataset it took 2 hours to do embedding with 20 workers. This model is fine-tuned from a variation of Microsoft MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model and embedding path is defined, load existing embedding and texts\n",
    "if model_name == 'multi-qa-MiniLM-L6-cos-v1':\n",
    "    embeddings_filepath = data_path + 'corpus_emb_MiniLM-L6.pkl'\n",
    "    if os.path.isfile(embeddings_filepath):\n",
    "        \n",
    "        with open(embeddings_filepath, \"rb\") as fIn:\n",
    "            cache_data = pickle.load(fIn)\n",
    "            passages = cache_data['sentences']\n",
    "            corpus_embeddings = cache_data['embeddings']\n",
    "            corpus_embeddings = corpus_embeddings.float()  # Convert embedding file to float\n",
    "# otherwise generate new embedding and store it to pickle        \n",
    "else: \n",
    "    corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True, show_progress_bar=True)\n",
    "    with open(working_path + 'corpus_emb_MiniLM-L6.pkl', \"wb\") as f:\n",
    "        pickle.dump({'sentences': passages, 'embeddings': corpus_embeddings}, f, protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a small corpus (up to 1 million documents), we can compute the cosine-similarity between query and documents in corpus by ` util.cos_sim() ` and retrieve top k documents by `torch.topk `. Fortunately this is done for us in `sentence_transformers.util.semantic_search(query_embeddings: torch.Tensor, corpus_embeddings: torch.Tensor, query_chunk_size: int = 100, corpus_chunk_size: int = 500000, top_k: int = 10, score_function: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = <function cos_sim>)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 1. Try it out</b> <br>\n",
    "Embed the quesiton and use the function util.semantic_search( ) and get top_k relevant documents. <br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input question: How many people Aileen Carol killed?\n",
      "\t0.608\t['Aileen Wuornos', 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.']\n",
      "\t0.549\t['Hanau', 'Nine people were killed in two shootings in Hanau on 19 February 2020.']\n",
      "\t0.526\t['Pauline Fowler', 'In the show, she lived at number 45 Albert Square. She had three children, Mark, Michelle and Martin. Mark died in 2004 of AIDS.']\n",
      "\t0.524\t['Diest', 'In 2007, 22845 people lived there.']\n",
      "\t0.517\t['Carol Ann Susi', 'On November 11, 2014, Susi died of cancer of unknown primary origin in Los Angeles, California, aged 62.']\n",
      "\n",
      "\n",
      "========\n",
      "\n",
      "CPU times: user 490 ms, sys: 257 ms, total: 747 ms\n",
      "Wall time: 749 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "top_k = 5\n",
    "\n",
    "query = 'How many people Aileen Carol killed?'\n",
    "### TODO \n",
    "query_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=top_k)\n",
    "\n",
    "hits = hits[0]\n",
    "print(\"Input question:\", query)\n",
    "for hit in hits:\n",
    "    print(\"\\t{:.3f}\\t{}\".format(hit['score'], passages[hit['corpus_id']]))\n",
    "print(\"\\n\\n========\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pack above code into a function to use later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 2.</b> <br>\n",
    "Write the function searchContext( ): <br>\n",
    "1. Encode the query<br>\n",
    "2. Perform semantic search between question embedding and corpus embedding<br>\n",
    "3. Return top_k hit <b>passages indexes</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchContext(top_k, bi_encoder, query, corpus_embedding):\n",
    "    indexes = []\n",
    "    t = time.time()\n",
    "    ### TODO\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]\n",
    "    for hit in hits:\n",
    "        indexes.append(hit['corpus_id'])\n",
    "\n",
    "    print('>>>> Relevent document search finished in : {}s'.format(time.time()-t))\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution</b></summary>\n",
    "\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]\n",
    "    for hit in hits:\n",
    "        indexes.append(hit['corpus_id'])\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper function to get the context from top hit ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the top ranked passages to feed BERT\n",
    "def getContext(indexes, passages):\n",
    "    contexts = []\n",
    "    for k in indexes:\n",
    "        contexts.append(passages[k][1])\n",
    "    return contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Widget 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 3.</b> <br>\n",
    "Complete the function QandA( ): <br>\n",
    "1. Extract top matching document ids, get the contexts and answers<br>\n",
    "2. display original documents that are matched<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QandA(corpus_embeddings, passages):\n",
    "    # Ask for question input\n",
    "    promptQ = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:')\n",
    "    display(promptQ)\n",
    "    question = input()\n",
    "    questions = [question]\n",
    "\n",
    "    # ask for top k value\n",
    "    promptK = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>')\n",
    "    display(promptK)\n",
    "    top_k = int(input())\n",
    "\n",
    "    # display question\n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: '+question+'</div>'\n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    ### TODO 3.1 search the corpus for relevent passages and answers\n",
    "    top_k_ids = searchContext(top_k, bi_encoder, question, corpus_embeddings)\n",
    "    contexts = getContext(top_k_ids, passages)\n",
    "    answers = getAnswer(contexts, questions, tokenizer, qa_model)\n",
    "    \n",
    "    for a in answers:\n",
    "        answers_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: '+a+'</div>'\n",
    "        display(HTML(answers_HTML))\n",
    "    # warning text\n",
    "    warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>'\n",
    "    display(HTML(warning_HTML))\n",
    "    \n",
    "    ### TODO 3.2 show original documents\n",
    "    doc = [passages[k] for k in top_k_ids]\n",
    "    df_hits = pd.DataFrame(doc, columns=['title','text'])\n",
    "    df_hits.text.str.wrap(100)\n",
    "    display(HTML(df_hits.to_html(render_links=True, escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " who is xi jingping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: who is xi jingping</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Relevent document search finished in : 0.7492396831512451s\n",
      ">>>> Looking for answers in 3 documents...\n",
      ">>>> Answers extracted in : 2.0815374851226807s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: xi jinping</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: a chinese politician who is currently the general secretary of the communist party of china</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: the son of chinese communist veteran xi zhongxun and qi xin</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xi Zhongxun</td>\n",
       "      <td>Xi is also the father of Xi Jinping, the current General Secretary of the Communist Party and President of China and also Chairman of the Military Commission.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>Xi Jinping (; born 15 June 1953) is a Chinese politician who is currently the General Secretary of the Communist Party of China (CPC), the President of the People's Republic of China, and the Chairman of the Central Military Commission. As General Secretary, he is also a member of the CPC Politburo Standing Committee, China's top decision-making body.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>Xi Jinping is the son of Chinese communist veteran Xi Zhongxun and Qi Xin. He rose politically in China's coastal provinces. He was the Governor of Fujian between 1999 and 2002. Between 2002 and 2007, he was Governor and CPC party chief of Zhejiang. After the dismissal of Chen Liangyu, Xi was transferred to Shanghai as the party secretary for a short time in 2007. Xi was promoted to the central leadership in October 2007 and trained to become Hu Jintao's successor.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QandA(corpus_embeddings, passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution</b></summary>\n",
    "    \n",
    "    ### TODO 3.1 \n",
    "    top_k_ids = searchContext(top_k, bi_encoder, question, corpus_embeddings)\n",
    "    contexts = getContext(top_k_ids, passages)\n",
    "    answers = getAnswer(contexts, questions, tokenizer, model)\n",
    "    ### TODO 3.2 \n",
    "    doc = [passages[k] for k in top_k_ids]\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Improve The Widget  \n",
    "### Retrieve & Re-rank with Cross-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The are some different ways to improve this workflow. Here we introduce the **Retrieve & Re-rank Pipeline**, which provides better performance for long docment and complex searches. It retrieves the relevant passages first using bi-encoder (what we did above), then re-rank them by the classification score between each pair of question and passage.  \n",
    "![bi-cross encode](../img/Bi_vs_Cross-Encoder.png )   \n",
    "image from: https://www.sbert.net/examples/applications/cross-encoder/README.html  \n",
    "Cross-encoders do not produce embeddings, so it is less efficient for comparision with millions of pairs data. However, in this pipeline, we limit our scope using bi-encoder first and then use cross-encoder to improve the accuracy of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "# downloading from library\n",
    "# cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "# cross_encoder.save('../model/cross_encoder')\n",
    "cross_encoder = CrossEncoder(model_path + 'cross_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.458562 ,   9.398578 ,  -3.1490183, -11.279931 ,   8.263057 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "query = 'When is Aileen Wuornos born'\n",
    "\n",
    "# The cross-encoder takes 2 inputs and perform classification tasks\n",
    "cross_input = [[query, 'Ted Cassidy (July 31, 1932 - January 16, 1979) was an American actor. He was best known for his roles as Lurch and Thing on \"The Addams Family\".'],\n",
    "                [query, 'Aileen Carol Wuornos Pralle (born Aileen Carol Pittman; February 29, 1956\\xa0– October 9, 2002) was an American serial killer. She was born in Rochester, Michigan. She confessed to killing six men in Florida and was executed in Florida State Prison by lethal injection for the murders. Wuornos said that the men she killed had raped her or tried to rape her while she was working as a prostitute.'],\n",
    "                [query, 'Wuornos was diagnosed with antisocial personality disorder and borderline personality disorder.'],\n",
    "                [query, 'The movie, \"Monster\" is about her life. Two documentaries were made about her.'], \n",
    "                [query, 'Wuornos was born Aileen Carol Pittman in Rochester, Michigan. She never met her father. Wuornos was adopted by her grandparents. When she was 13 she became pregnant. She started working as a prostitute when she was 14.']]\n",
    "\n",
    "cross_scores = cross_encoder.predict(cross_input)\n",
    "cross_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Task 4: Complete function rerank( ) and QandArerank( )</b> <br>\n",
    "    1.  The function takes one question and compares with a list of contexts. We use the list of indexs and passages data to get the context. The list of indexes will be passed by bi-encoder search. The return will be indexes list sorted by cross-encoder score.<br>\n",
    "    2. use bi-encoder to search for top 20 relevant passages and use rerank( ) to get the top_k passage indexes. top_k is the user input number.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, indexs, passages):\n",
    "    print('>>>> Re-ranking results...')\n",
    "    ### TODO 4.1\n",
    "    cross_in = [[question, passages[idx][0]] for idx in indexs]\n",
    "    cross_result = cross_encoder.predict(cross_in)\n",
    "    rank_top = []\n",
    "    for i, v in enumerate(indexs):\n",
    "        rank_top.append([v, cross_result[i]])\n",
    "    rank_top = sorted(rank_top, key=lambda x: x[1], reverse=True)\n",
    "    rank_top = [e[0] for e in rank_top]\n",
    "    return rank_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QandArerank(corpus_embeddings, passages):\n",
    "    # Ask for question input\n",
    "    promptQ = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:')\n",
    "    display(promptQ)\n",
    "    question = input()\n",
    "    questions = [question]\n",
    "\n",
    "    # ask for top k value after cross-encoder\n",
    "    promptK = HTML('<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>')\n",
    "    display(promptK)\n",
    "    top_k = int(input())\n",
    "\n",
    "    # display question\n",
    "    question_HTML = '<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: '+question+'</div>'\n",
    "    display(HTML(question_HTML))\n",
    "    \n",
    "    ### TODO 4.2 \n",
    "    bi_top_k = searchContext(20, bi_encoder, questions, corpus_embeddings)\n",
    "    cross_top_k = rerank(question, bi_top_k, passages)[:top_k]\n",
    "    contexts = getContext(cross_top_k, passages)\n",
    "    answers = getAnswer(contexts, questions, tokenizer, qa_model)\n",
    "    \n",
    "    for a in answers:\n",
    "        answers_HTML = '<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: '+a+'</div>'\n",
    "        display(HTML(answers_HTML))\n",
    "    # warning text\n",
    "    warning_HTML = '<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>'\n",
    "    display(HTML(warning_HTML))\n",
    "    \n",
    "    doc = [passages[k] for k in cross_top_k]\n",
    "    \n",
    "    df_hits = pd.DataFrame(doc, columns=['title','text'])\n",
    "    df_hits.text.str.wrap(100)\n",
    "    display(HTML(df_hits.to_html(render_links=True, escape=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Ask me a question</b>:"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " who is xi jingping\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>How many results would you like?</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " 5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 20px; padding-bottom:28px; margin-top:1pt\"><b>Query</b>: who is xi jingping</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Relevent document search finished in : 0.842926025390625s\n",
      ">>>> Re-ranking results...\n",
      ">>>> Looking for answers in 5 documents...\n",
      ">>>> Answers extracted in : 3.3726346492767334s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: a chinese politician who is currently the general secretary of the communist party of china</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: the son of chinese communist veteran xi zhongxun and qi xin</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: xi stopped this tradition and abandoned his potential successors hu chunhua and sun zhengcai</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 18px; margin-bottom:1pt\"><b>Answer found</b>: zhu xi</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-family: Times New Roman; font-size: 15px; padding-bottom:15px; color:#E76f51; margin-top:1pt\"> These are extracted answers from original documents. Please see the documents below:</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>Xi Jinping (; born 15 June 1953) is a Chinese politician who is currently the General Secretary of the Communist Party of China (CPC), the President of the People's Republic of China, and the Chairman of the Central Military Commission. As General Secretary, he is also a member of the CPC Politburo Standing Committee, China's top decision-making body.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>Xi Jinping is the son of Chinese communist veteran Xi Zhongxun and Qi Xin. He rose politically in China's coastal provinces. He was the Governor of Fujian between 1999 and 2002. Between 2002 and 2007, he was Governor and CPC party chief of Zhejiang. After the dismissal of Chen Liangyu, Xi was transferred to Shanghai as the party secretary for a short time in 2007. Xi was promoted to the central leadership in October 2007 and trained to become Hu Jintao's successor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>Xi was born on 15 June 1953 in Beijing, China. His father held lots of posts, including party propaganda chief and vice premier. He has been married to Peng Liyuan since 1987. They have one daughter, Xi Mingze, who graduated from Harvard University in 2015. Xi lives in Zhongnanhai, China.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xi Jinping</td>\n",
       "      <td>By tradition in recent decades, the Chinese leader leads two terms (10 years in total). The second term identifies his successor and prepares for the power transfer. However, Xi stopped this tradition and abandoned his potential successors Hu Chunhua and Sun Zhengcai, who were put to prison in 2018 due to corruption.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zhu Xi</td>\n",
       "      <td>Zhu Xi or Chu Hsi (, 18 October 1130 – 23 April 1200) was a Confucian scholar during the Song Dynasty. Zhu Xi was one of the three most important Confucian philosophers. He organized the classic works of Confucianism and contributed to the philosophy of Neo-Confucianism.He was from Fujian province in China.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QandArerank(corpus_embeddings, passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><b>Solution</b></summary>\n",
    "    \n",
    "    ### TODO 4.1 \n",
    "    cross_in = [[question, passages[idx][0]] for idx in indexs]\n",
    "    cross_result = cross_encoder.predict(cross_in)\n",
    "\n",
    "    ### TODO 4.2 show original documents\n",
    "    bi_top_k = searchContext(20, bi_encoder, questions, corpus_embeddings)\n",
    "    cross_top_k = rerank(question, bi_top_k, passages)[:top_k]\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Explore the dataset and ask different questions to see the performance  \n",
    "\n",
    ">Pay attention to the **first** answer in both of your widgets, can you see the improvement?  \n",
    "\n",
    ">Change to bigger dataset for longer documents, and try these widgets at home!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Apart from the retrieve-re-rank pipeline, there is also other methods like using Elasticsearch, FAISS indexing or nearest neightbours to improve the workflow. It depends on the size of dataset and the task you wish to perform.  \n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------  \n",
    "By the end of this notebook, you have created and improved a small widget to find answers for you in the articles, i.e information retrieval. In the [next notebook](4-Topic_Modelling.ipynb), we will have a look at topic modelling, another useful application to explore a large amount of text data.    \n",
    "\n",
    "--------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/jobfs/94897624.gadi-pbs/ipykernel_590816/434258622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# download datasets and models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'squad'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-cased'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "# download datasets and models\n",
    "import datasets\n",
    "dataset = datasets.load_dataset('squad')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-cased', return_dict=True)\n",
    "metric = datasets.load_metric('squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may encounter some warning when you download a new model from HuggingFace. The warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the pre_classifier and classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.  \n",
    "\n",
    "Now let's try the tokenizer and inspect our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Processing long contexts**  \n",
    "The model from Hugging Face has the maximum input length of 512, which includes the question and the context. So we need to make some changes in our `tokenizer` configuration.  \n",
    "\n",
    "In the last notebook we truncted sequences that are too long, but here we risk losing the answer if we simply cut the extra text off. So we allow a long context to provide multiple input features, each of them has a length within the input limit (`max_length`). In case the answer is at the splitting point in the context, we also allow certain length of overlap (`doc_stride`) between features for the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this find a long context example\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > max_length:\n",
    "        break\n",
    "example = dataset[\"train\"][i]\n",
    "# Without any truncation, we get the following length for the input IDs:\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we never want to truncate the question, only the context, so the `only_second` truncation picked. Now, our tokenizer can automatically return us a list of features capped by a certain maximum length, with the overlap we talked above, we just have to tell it with `return_overflowing_tokens=True` and passing the `stride`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets's see the truncted context with overlaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model reuqires the start positon and end positions of answers in the tokens. So we need to map parts of the original context to tokens by setting `return_offsets_mapping=True`. The very first token ([CLS]) has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 3 of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we distinguish which parts of the offsets are for question and which parts are for context by using `sequence_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    \n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In the case of several examples, the tokenizer will return a list of lists for each key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = prepare_train_features(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the function over the entire dataset\n",
    "tokenized_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset['train'].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('../model/ft-squad',# directory to save trained model\n",
    "                        evaluation_strategy='epoch',\n",
    "                        learning_rate=0.001,\n",
    "                        per_device_train_batch_size=4000,\n",
    "                        per_device_eval_batch_size=4000,\n",
    "                        num_train_epochs=2,\n",
    "                        # weight_decay=0.01,\n",
    "                        )\n",
    "trainer = Trainer(model, args,\n",
    "                 train_dataset=tokenized_datasets['train'], # for time sake we only use part of the dataset in workshop, feel free to use full traning dataset later! \n",
    "                 eval_dataset=tokenized_datasets['validation'],\n",
    "                 data_collator=default_data_collator,tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model after a long training\n",
    "trainer.save_model(\"../model/tuned-squad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4f5d7df9eebeb2fce5c7cb4fadb86274017838dfb7de8d5dd5849e5abb02796"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
